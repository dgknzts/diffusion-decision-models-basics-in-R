---
title: "DDM Parameter Recovery: A Step-by-Step Tutorial"
author: "Dogukan Nami Oztas"
date: "2025-05-24"
order: 8
output:
  html_document:
    toc: true
    toc_float: true
    theme: united 
    highlight: tango
    df_print: kable
vignette: >
  %\VignetteIndexEntry{DDM Parameter Recovery}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = 'center')
library(dplyr)
library(ggplot2)
library(knitr)

# Source the functions we need for DDM simulation and fitting
source("../R/03_ddm_simulator_variable.R")
source("../R/05_ddm_advanced_fitting.R")
```

## Introduction to Parameter Recovery

**Parameter recovery** is a crucial validation step in computational modeling. The question is: "If we know the true parameters that generated some data, can our fitting procedure recover those same parameters?"

This vignette demonstrates how to:

1.  **Generate data** with known DDM parameters
2.  **Set up optimization** to find the best-fitting parameters
3.  **Evaluate recovery** by comparing estimated vs. true parameters
4.  **Use multiple optimization starts** to avoid local minima

## Why is Parameter Recovery Important?

Before applying DDM fitting to real experimental data, we need confidence that our method works. Parameter recovery testing helps us:

-   **Validate our fitting procedure**: Does it work when we know the answer?
-   **Understand parameter identifiability**: Which parameters are easy/hard to recover?
-   **Set realistic expectations**: How much error should we expect?
-   **Debug fitting problems**: If recovery fails, there's an issue with our method

## Step 1: Generate Target Data with Known Parameters

Let's start by generating DDM data with parameters we want to recover:

```{r generate_target_data}
# Define the "true" parameters that we'll try to recover
true_parameters <- list(
  mean_v   = 0.15,    # Drift rate
  a        = 0.8,     # Threshold  
  mean_z   = 0.4,     # Starting point
  s        = 0.3,     # Within-trial noise
  mean_ter = 0.12,    # Non-decision time
  sv       = 0.1,    # Drift rate variability
  sz       = 0.02,    # Starting point variability  
  st0      = 0.03     # Non-decision time variability
)

cat("True Parameters for Recovery Test:\n")
print(unlist(true_parameters))

# Generate target dataset with these parameters
set.seed(12345)  # For reproducibility
target_data <- simulate_diffusion_experiment_variable(
  n_trials = 1000,
  mean_v   = true_parameters$mean_v,
  a        = true_parameters$a,
  mean_z   = true_parameters$mean_z,
  s        = true_parameters$s,
  dt       = 0.01,
  mean_ter = true_parameters$mean_ter,
  sv       = true_parameters$sv,
  sz       = true_parameters$sz,
  st0      = true_parameters$st0
)

cat("\nGenerated", nrow(target_data), "trials for parameter recovery test\n")

# Quick summary of the target data
summary_stats <- target_data %>%
  filter(!is.na(choice) & !is.na(rt)) %>%
  summarise(
    n_trials = n(),
    prop_correct = mean(choice == 1),
    mean_rt_correct = mean(rt[choice == 1], na.rm = TRUE),
    mean_rt_error = mean(rt[choice == 0], na.rm = TRUE),
    .groups = 'drop'
  )

kable(summary_stats, caption = "Target Data Summary", digits = 3)
```

## Step 2: Prepare Data for Fitting

Our fitting procedure uses binned reaction time distributions. This means we divide the RT range into bins and count how many responses fall into each bin:

```{r prepare_binned_data}
# Create RT bins for fitting
max_rt <- max(target_data$rt, na.rm = TRUE)
rt_bins <- seq(0, ceiling(max_rt * 1.2), by = 0.1)

cat("RT bins:", length(rt_bins) - 1, "bins from", min(rt_bins), "to", max(rt_bins), "seconds\n")

# Calculate binned proportions from target data
target_binned_props <- calculate_binned_rt_proportions(
  target_data,
  rt_bins = rt_bins
)

cat("\nBinned target data:\n")
kable(head(target_binned_props, 10), caption = "Sample of Binned Target Data")
```

## Step 3: Set Up Parameter Recovery

Now we define which parameters to optimize and their constraints:

```{r setup_optimization}
# Parameters we want to recover
param_names <- c("mean_v", "a", "mean_z", "s", "mean_ter", "sv", "sz", "st0")

# Starting values for optimization (we'll use multiple random starts)
initial_means <- c(
  mean_v   = 0.10,   # Start near but not exactly at true values
  a        = 0.9,    
  mean_z   = 0.45,   
  s        = 0.12,   
  mean_ter = 0.10,   
  sv       = 0.05,   
  sz       = 0.015,  
  st0      = 0.025   
)

# Variability around starting values
initial_sds <- c(
  mean_v   = 0.03,   # Reasonable search ranges
  a        = 0.1,    
  mean_z   = 0.05,   
  s        = 0.02,   
  mean_ter = 0.02,   
  sv       = 0.02,   
  sz       = 0.01,   
  st0      = 0.01    
)

# Parameter bounds
lower_bounds <- c(
  mean_v   = -0.5,   # Drift can be negative
  a        = 0.3,    # Threshold must be positive
  mean_z   = 0.1,    # Starting point must be within bounds
  s        = 0.05,   # Noise must be positive
  mean_ter = 0.01,   # Non-decision time must be positive
  sv       = 0,      # Variability parameters must be non-negative
  sz       = 0,      
  st0      = 0       
)

upper_bounds <- c(
  mean_v   = 0.8,    
  a        = 1.5,    
  mean_z   = 0.7,    # Starting point must be less than threshold
  s        = 0.3,    
  mean_ter = 0.25,   
  sv       = 0.3,    
  sz       = 0.1,    
  st0      = 0.1     
)

# Fixed parameters for simulation
fixed_params <- list(
  dt = 0.01,
  correct_choice_value = 1,
  error_choice_value = 0
)

cat("Optimization setup complete.\n")
cat("Parameters to recover:", paste(param_names, collapse = ", "), "\n")
```

## Step 4: Run Parameter Recovery

We'll use multiple optimization starts to increase our chances of finding the global minimum:

```{r run_optimization, cache=TRUE}
cat("Starting parameter recovery with multiple optimization runs...\n")

# Run optimization with multiple random starts
recovery_results <- run_ddm_optimization_multi_start(
  n_starts = 3,  # Number of different starting points
  target_stats_or_binned_props = target_binned_props,
  param_names_optim = param_names,
  initial_guesses_means = initial_means,
  initial_guesses_sds = initial_sds,
  lower_bounds = lower_bounds,
  upper_bounds = upper_bounds,
  objective_fn_name = "ddm_binned_likelihood_objective",
  n_sim_per_eval = 500,  # Trials simulated per optimization step
  fixed_params = fixed_params,
  optim_maxit = 100,      # Maximum optimization iterations
  rt_bins = rt_bins,
  constrain_z_to_a_div_2 = FALSE,  # Allow mean_z to be optimized independently
  verbose = FALSE
)

cat("Parameter recovery completed!\n")
```

## Step 5: Evaluate Recovery Results

Let's examine how well we recovered the true parameters:

```{r evaluate_recovery}
# Extract the best parameter estimates
best_estimates <- recovery_results$best_optim_result$par
names(best_estimates) <- param_names

# Create comparison table
recovery_comparison <- data.frame(
  Parameter = param_names,
  True_Value = sapply(param_names, function(p) true_parameters[[p]]),
  Estimated_Value = best_estimates,
  Absolute_Error = abs(best_estimates - sapply(param_names, function(p) true_parameters[[p]])),
  Percent_Error = abs(best_estimates - sapply(param_names, function(p) true_parameters[[p]])) / 
                  sapply(param_names, function(p) true_parameters[[p]]) * 100
)

kable(recovery_comparison, 
      caption = "Parameter Recovery Results", 
      digits = 4)

# Summary of recovery quality
cat("\n=== Recovery Quality Summary ===\n")
cat("Best objective function value:", round(recovery_results$best_optim_result$value, 2), "\n")
cat("Optimization convergence:", recovery_results$best_optim_result$convergence, "\n")

# Calculate overall recovery metrics
mean_abs_error <- mean(recovery_comparison$Absolute_Error)
mean_pct_error <- mean(recovery_comparison$Percent_Error)

cat("\nOverall Recovery Performance:\n")
cat("Mean absolute error:", round(mean_abs_error, 4), "\n")
cat("Mean percentage error:", round(mean_pct_error, 1), "%\n")

# Show which parameters were recovered well vs. poorly
well_recovered <- recovery_comparison$Parameter[recovery_comparison$Percent_Error < 15]
poorly_recovered <- recovery_comparison$Parameter[recovery_comparison$Percent_Error >= 15]

cat("\nWell-recovered parameters (<15% error):", paste(well_recovered, collapse = ", "), "\n")
cat("Challenging parameters (â‰¥15% error):", paste(poorly_recovered, collapse = ", "), "\n")
```

## Step 6: Visualize Recovery Results

Let's create a visual comparison of true vs. estimated parameters:

```{r visualize_recovery, fig.width=10, fig.height=6}
# Create visualization
recovery_plot <- ggplot(recovery_comparison, aes(x = Parameter)) +
  geom_point(aes(y = True_Value, color = "True"), size = 3) +
  geom_point(aes(y = Estimated_Value, color = "Estimated"), size = 3) +
  geom_segment(aes(xend = Parameter, y = True_Value, yend = Estimated_Value),
               alpha = 0.7, linetype = "dashed") +
  scale_color_manual(values = c("True" = "blue", "Estimated" = "red")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Parameter Recovery Results",
       subtitle = "Blue = True Values, Red = Estimated Values",
       y = "Parameter Value",
       color = "Type")

print(recovery_plot)
```

## Understanding Recovery Results

### What Makes Parameters Easy or Hard to Recover?

**Typically Well-Recovered Parameters:** - **Threshold (a)**: Controls the overall speed-accuracy tradeoff - **Drift rate (mean_v)**: Determines choice bias and mean RT - **Starting point (mean_z)**: Affects response bias

**Typically Challenging Parameters:** - **Variability parameters (sv, sz, st0)**: Hard to distinguish from noise - **Within-trial noise (s)**: Often confounded with other parameters

### Why Multiple Optimization Starts?

Let's see how different starting points performed:

```{r show_all_runs}
# Show results from all optimization runs
all_runs <- recovery_results$all_runs_summary %>%
  select(run, obj_value, convergence, starts_with("estimated_")) %>%
  arrange(obj_value)

kable(head(all_runs), 
      caption = "All Optimization Runs (sorted by objective value)", 
      digits = 4)

cat("Range of objective values across runs:", 
    round(min(all_runs$obj_value), 2), "to", 
    round(max(all_runs$obj_value), 2), "\n")
```

Multiple starts help because: - **Local minima**: Optimization can get stuck in suboptimal solutions - **Parameter interactions**: Some parameter combinations create complex landscapes - **Random variation**: Simulation noise can mislead optimization

## Best Practices for Parameter Recovery

### 1. **Use Sufficient Data**

-   More trials â†’ more stable parameter estimates
-   Recommended: 1000+ trials for basic recovery
-   3000+ trials for variability parameters

### 2. **Multiple Optimization Starts**

-   Use 5-10 different starting points
-   Compare results across runs
-   Choose the best objective function value

### 3. **Realistic Parameter Bounds**

-   Set bounds based on theoretical constraints
-   Don't make bounds too tight (prevents exploration)
-   Don't make bounds too loose (allows unrealistic values)

### 4. **Evaluate Recovery Quality**

-   Always test on simulated data first
-   Check which parameters are identifiable
-   Set realistic expectations for real data

## Summary

This tutorial demonstrated the complete parameter recovery workflow:

1.  âœ… **Generated target data** with known parameters
2.  âœ… **Set up optimization** with appropriate bounds and starting values\
3.  âœ… **Used multiple starts** to find global minimum
4.  âœ… **Evaluated recovery quality** through direct comparison
5.  âœ… **Identified** which parameters are easy/hard to recover

**Key takeaway**: Parameter recovery testing is essential before fitting real data. It helps validate your method and set realistic expectations for recovery accuracy.

### Next Steps

-   Apply this approach to your own experimental data
-   Experiment with different sample sizes and parameter values
-   Consider which parameters are most important for your research questions
-   Use parameter recovery results to guide data collection planning
